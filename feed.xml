<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://kangdongho.me/feed.xml" rel="self" type="application/atom+xml"/><link href="https://kangdongho.me/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-08-14T21:54:37+00:00</updated><id>https://kangdongho.me/feed.xml</id><title type="html">DONGHO KANG</title><subtitle>I am a doctoral student at ETH Zurich and a research assistant at Computational Robotics Lab. My research aims to create a legged robot that can perform natural, animal-like motion. Thus, my research interests are broad ranging to legged locomotion control, computational model of character animation and design optimization for robotics applications. </subtitle><entry><title type="html">Learning Steerable Imitation Controllers From Unstructured Animal Motions</title><link href="https://kangdongho.me/learning-steerable-imitation-controllers/" rel="alternate" type="text/html" title="Learning Steerable Imitation Controllers From Unstructured Animal Motions"/><published>2025-07-02T22:39:00+00:00</published><updated>2025-07-02T22:39:00+00:00</updated><id>https://kangdongho.me/learning-steerable-imitation-controllers</id><content type="html" xml:base="https://kangdongho.me/learning-steerable-imitation-controllers/"><![CDATA[<div class="row"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid" src="/assets/img/kang2025learning/teaser.jpg" alt="" title="teaser"/> </div> </div> <p><br/></p> <h1 id="abstract">Abstract</h1> <p>This paper presents a control framework for legged robots that leverages unstructured real-world animal motion data to generate animal-like and user-steerable behaviors. Our framework learns to follow velocity commands while reproducing the diverse gait patterns in the original dataset. To begin with, animal motion data is transformed into a robot-compatible database using constrained inverse kinematics and model predictive control, bridging the morphological and physical gap between the animal and the robot. Subsequently, a variational autoencoder-based motion synthesis module captures the diverse locomotion patterns in the motion database and generates smooth transitions between them in response to velocity commands. The resulting kinematic motions serve as references for a reinforcement learning-based feedback controller deployed on physical robots. We show that this approach enables a quadruped robot to adaptively switch gaits and accurately track user velocity commands while maintaining the stylistic coherence of the motion data. Additionally, we provide component-wise evaluations to analyze the system’s behavior in depth and demonstrate the efficacy of our method for more accurate and reliable motion imitation.</p> <p><strong>Open access: [<a href="https://arxiv.org/abs/2507.00677">ArXiv</a>]</strong></p> <hr/> <h1 id="supplementary-video">Supplementary Video</h1> <div class="embed-responsive embed-responsive-16by9"> <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/DukyUGNYf5A" allowfullscreen=""></iframe> </div> <hr/> <h1 id="demos">Demos</h1> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/kang2025learning/sit-pace-sit.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls="" loop=""/> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/kang2025learning/spin.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls="" loop=""/> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/kang2025learning/gallop.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls="" loop=""/> </figure> </div> </div> <div class="caption"> Prerecorded dog motion sequences to a real-world robot: <i>Sit-pace-sit</i> (first), <i>Spin</i> (second) and <i>Gallop</i> (third) </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/kang2025learning/transitions.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls="" loop=""/> </figure> </div> </div> <div class="caption"> <i>Unitree Go2</i> robot adaptively changing its gait pattern based on the forward velocity command. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/kang2025learning/steerable-locomotion.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls="" loop=""/> </figure> </div> </div> <div class="caption"> <i>Unitree Go2</i> robot navigating freely across a grass field in response to joystick commands. </div> <hr/> <h1 id="bibtex">Bibtex</h1> <figure class="highlight"><pre><code class="language-txt" data-lang="txt">@misc{kang2025learningsteerableimitationcontrollers,
  title={Learning Steerable Imitation Controllers from Unstructured Animal Motions}, 
  author={Dongho Kang and Jin Cheng and Fatemeh Zargarbashi and Taerim Yoon and Sungjoon Choi and Stelian Coros},
  year={2025},
  eprint={2507.00677},
  archivePrefix={arXiv},
  primaryClass={cs.RO},
  url={https://arxiv.org/abs/2507.00677}, 
}</code></pre></figure> ]]></content><author><name>Dongho Kang</name></author><category term="publication"/><category term="robotics"/><category term="character animation"/><summary type="html"><![CDATA[ArXiv]]></summary></entry><entry><title type="html">Tuning Legged Locomotion Controllers via Safe Bayesian Optimization</title><link href="https://kangdongho.me/tuning-legged-locomotion-controllers/" rel="alternate" type="text/html" title="Tuning Legged Locomotion Controllers via Safe Bayesian Optimization"/><published>2023-06-08T13:58:00+00:00</published><updated>2023-06-08T13:58:00+00:00</updated><id>https://kangdongho.me/tuning-legged-locomotion-controllers</id><content type="html" xml:base="https://kangdongho.me/tuning-legged-locomotion-controllers/"><![CDATA[<div class="row"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid" src="/assets/img/widmer2023tuning/widmer2023tuning.png" alt="" title="teaser"/> </div> </div> <p><br/></p> <h1 id="abstract">Abstract</h1> <p>This paper presents a data-driven strategy to streamline the deployment of model-based controllers in legged robotic hardware platforms. Our approach leverages a model-free safe learning algorithm to automate the tuning of control gains, addressing the mismatch between the simplified model used in the control formulation and the real system. This method substantially mitigates the risk of hazardous interactions with the robot by sample-efficiently optimizing parameters within a probably safe region. Additionally, we extend the applicability of our approach to incorporate the different gait parameters as contexts, leading to a safe, sample-efficient exploration algorithm capable of tuning a motion controller for diverse gait patterns. We validate our method through simulation and hardware experiments, where we demonstrate that the algorithm obtains superior performance on tuning a model-based motion controller for multiple gaits safely.</p> <p><strong>Paper: [<a href="https://proceedings.mlr.press/v229/widmer23a.html">PMLR</a>]</strong>   <strong>Open access: [<a href="https://arxiv.org/abs/2306.07092">ArXiv</a>]</strong>   <strong>Code: [<a href="https://github.com/lasgroup/gosafeopt">GitHub</a>]</strong></p> <hr/> <h1 id="full-supplementary-video">Full Supplementary Video</h1> <div class="embed-responsive embed-responsive-16by9"> <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/zVm7IkYofbg" allowfullscreen=""></iframe> </div> <hr/> <h1 id="bibtex">Bibtex</h1> <figure class="highlight"><pre><code class="language-txt" data-lang="txt">@InProceedings{pmlr-v229-widmer23a,
  title = {Tuning Legged Locomotion Controllers via Safe Bayesian Optimization},
  author = {Widmer, Daniel and Kang, Dongho and Sukhija, Bhavya and H\"{u}botter, Jonas and Krause, Andreas and Coros, Stelian},
  booktitle = {Proceedings of The 7th Conference on Robot Learning},
  pages = {2444--2464},
  year = {2023},
  editor = {Tan, Jie and Toussaint, Marc and Darvish, Kourosh},
  volume = {229},
  series = {Proceedings of Machine Learning Research},
  month = {06--09 Nov},
  publisher = {PMLR}
}</code></pre></figure> <hr/> <h1 id="acknowledgment">Acknowledgment</h1> <p>We would like to thank Lenart Treven and Flavio De Vincenti for their feedback on this work.</p> <p>This project has received funding from the Swiss National Science Foundation under NCCR Automation, grant agreement 51NF40 180545, the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme, grant agreement No. 866480, and the Microsoft Swiss Joint Research Center.</p>]]></content><author><name>Daniel Widmer&lt;sup&gt;*&lt;/sup&gt;&lt;d-footnote&gt;* The first two authors contributed equally.&lt;/d-footnote&gt;</name></author><category term="publication"/><category term="robotics"/><summary type="html"><![CDATA[7th Annual Conference on Robot Learning (CoRL 2023)]]></summary></entry><entry><title type="html">RL + Model-based Control: Using On-demand Optimal Control to Learn Versatile Legged Locomotion</title><link href="https://kangdongho.me/rl-plus-model-based-control/" rel="alternate" type="text/html" title="RL + Model-based Control: Using On-demand Optimal Control to Learn Versatile Legged Locomotion"/><published>2023-05-27T16:10:33+00:00</published><updated>2023-05-27T16:10:33+00:00</updated><id>https://kangdongho.me/rl-plus-model-based-control</id><content type="html" xml:base="https://kangdongho.me/rl-plus-model-based-control/"><![CDATA[<div class="row"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid" src="/assets/img/kang2023rl/kang2023rl.png" alt="" title="teaser"/> </div> </div> <p><br/></p> <h1 id="abstract">Abstract</h1> <p>This letter presents a control framework that combines model-based optimal control and reinforcement learning (RL) to achieve versatile and robust legged locomotion. Our approach enhances the RL training process by incorporating on-demand reference motions generated through finite-horizon optimal control, covering a broad range of velocities and gaits. These reference motions serve as targets for the RL policy to imitate, leading to the development of robust control policies that can be learned with reliability. Furthermore, by utilizing realistic simulation data that captures whole-body dynamics, RL effectively overcomes the inherent limitations in reference motions imposed by modeling simplifications. We validate the robustness and controllability of the RL training process within our framework through a series of experiments. In these experiments, our method showcases its capability to generalize reference motions and effectively handle more complex locomotion tasks that may pose challenges for the simplified model, thanks to RL’s flexibility. Additionally, our framework effortlessly supports the training of control policies for robots with diverse dimensions, eliminating the necessity for robot-specific adjustments in the reward function and hyperparameters.</p> <p><strong>Paper: [<a href="https://ieeexplore.ieee.org/document/10225268">IEEE Xplore</a>]</strong>   <strong>Open access: [<a href="https://arxiv.org/abs/2305.17842">ArXiv</a>]</strong></p> <hr/> <h1 id="supplementary-video">Supplementary Video</h1> <div class="embed-responsive embed-responsive-16by9"> <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/HXwLXdOf79c" allowfullscreen=""></iframe> </div> <hr/> <h1 id="presentation">Presentation</h1> <p>We are pleased to announce that our paper has been selected for presentation at <a href="https://2024.ieee-icra.org/">ICRA 2024 in Yokohama, Japan</a>. Please join us for our oral presentation in the “Legged Robot III” session on May 14th from 16:30 to 18:00, and for our poster presentation session earlier that day from 10:30 to 12:00. We are excited to share our work with you and look forward to seeing you there.</p> <div class="embed-responsive embed-responsive-16by9"> <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/U4tCgsEFJmM" allowfullscreen=""></iframe> </div> <hr/> <h1 id="demos">Demos</h1> <div class="embed-responsive embed-responsive-16by9"> <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/PCT5f6xsASk" allowfullscreen=""></iframe> </div> <p><br/></p> <div class="embed-responsive embed-responsive-16by9"> <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/30CmkKIJ2fQ" allowfullscreen=""></iframe> </div> <p><br/></p> <div class="embed-responsive embed-responsive-16by9"> <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/-tUdeYsNvSQ" allowfullscreen=""></iframe> </div> <hr/> <h1 id="bibtex">Bibtex</h1> <figure class="highlight"><pre><code class="language-txt" data-lang="txt">@article{kang2023rl,
  author={Kang, Dongho and Cheng, Jin and Zamora, Miguel and Zargarbashi, Fatemeh and Coros, Stelian},
  journal={IEEE Robotics and Automation Letters}, 
  title={RL + Model-Based Control: Using On-Demand Optimal Control to Learn Versatile Legged Locomotion}, 
  year={2023},
  volume={8},
  number={10},
  pages={6619-6626},
  doi={10.1109/LRA.2023.3307008}
}</code></pre></figure> <hr/> <h1 id="acknowledgment">Acknowledgment</h1> <p>This work has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No. 866480.)</p> <p>We express our gratitude to Zijun Hui for his assistance with the robot experiments.</p>]]></content><author><name>Dongho Kang</name></author><category term="publication"/><category term="robotics"/><category term="journal"/><summary type="html"><![CDATA[IEEE Robotics and Automation Letters (RA-L)]]></summary></entry><entry><title type="html">Animal Motions on Legged Robots Using Nonlinear Model Predictive Control</title><link href="https://kangdongho.me/animal-motions-on-legged-robots/" rel="alternate" type="text/html" title="Animal Motions on Legged Robots Using Nonlinear Model Predictive Control"/><published>2022-07-31T15:12:33+00:00</published><updated>2022-07-31T15:12:33+00:00</updated><id>https://kangdongho.me/animal-motions-on-legged-robots</id><content type="html" xml:base="https://kangdongho.me/animal-motions-on-legged-robots/"><![CDATA[<div class="row"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid" src="/assets/img/kang2022animal/kang2022animal.png" alt="" title="teaser"/> </div> </div> <p><br/></p> <h1 id="abstract">Abstract</h1> <p>This work presents a motion capture-driven locomotion controller for quadrupedal robots that replicates the non-periodic footsteps and subtle body movement of animal motions. We adopt a nonlinear model predictive control (NMPC) formulation that generates optimal base trajectories and stepping locations. By optimizing both footholds and base trajectories, our controller effectively tracks retargeted animal motions with natural body movements and highly irregular strides. We demonstrate our approach with prerecorded animal motion capture data. In simulation and hardware experiments, our motion controller enables quadrupedal robots to robustly reproduce fundamental characteristics of a target animal motion regardless of the significant morphological disparity.</p> <p><strong>Paper: [<a href="https://ieeexplore.ieee.org/document/9981945">IEEE Xplore</a>]</strong>   <strong>Open access: [<a href="https://www.research-collection.ethz.ch/handle/20.500.11850/589749">ETH Research Collection</a>]</strong></p> <hr/> <h1 id="supplementary-video">Supplementary Video</h1> <div class="embed-responsive embed-responsive-16by9"> <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/TVV_GcNZ0Ts" allowfullscreen=""></iframe> </div> <hr/> <h1 id="presentation">Presentation</h1> <div class="embed-responsive embed-responsive-16by9"> <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/nv2VcuxtL4s" allowfullscreen=""></iframe> </div> <hr/> <h1 id="bibtex">Bibtex</h1> <figure class="highlight"><pre><code class="language-txt" data-lang="txt">@inproceedings{kang2022animal,
  title={Animal Motions on Legged Robots Using Nonlinear Model Predictive Control}, 
  author={Kang, Dongho and De Vincenti, Flavio and Adami, Naomi C. and Coros, Stelian},
  booktitle={2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  pages={11955-11962},
  year={2022},
  organization={IEEE},
  doi={10.1109/IROS47612.2022.9981945}
}</code></pre></figure> <hr/> <h1 id="acknowledgment">Acknowledgment</h1> <p>This work has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No. 866480.) The ethics have been approved by the veterinary authorities of the canton Zürich.</p>]]></content><author><name>Dongho Kang</name></author><category term="publication"/><category term="robotics"/><category term="conference"/><summary type="html"><![CDATA[IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2022)]]></summary></entry><entry><title type="html">Nonlinear Model Predictive Control for Quadrupedal Locomotion Using Second-Order Sensitivity Analysis</title><link href="https://kangdongho.me/nmpc-for-quad-loco/" rel="alternate" type="text/html" title="Nonlinear Model Predictive Control for Quadrupedal Locomotion Using Second-Order Sensitivity Analysis"/><published>2022-05-19T14:44:33+00:00</published><updated>2022-05-19T14:44:33+00:00</updated><id>https://kangdongho.me/nmpc-for-quad-loco</id><content type="html" xml:base="https://kangdongho.me/nmpc-for-quad-loco/"><![CDATA[<div class="row"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid" src="/assets/img/kang2021nonlinear/teaser.png" alt="" title="teaser"/> </div> </div> <p><br/></p> <h1 id="abstract">Abstract</h1> <p>We present a versatile nonlinear model predictive control (NMPC) formulation for quadrupedal locomotion. Our formulation jointly optimizes a base trajectory and a set of footholds over a finite time horizon based on simplified dynamics models. We leverage second-order sensitivity analysis and a sparse Gauss-Newton (SGN) method to solve the resulting optimal control problems. We further describe our ongoing effort to verify our approach through simulation and hardware experiments. Finally, we extend our locomotion framework to deal with challenging tasks that comprise gap crossing, movement on stepping stones, and multi-robot control.</p> <p><strong>Paper: [<a href="https://arxiv.org/abs/2207.10465">ArXiv</a>]</strong></p> <hr/> <h1 id="supplementary-video">Supplementary Video</h1> <div class="embed-responsive embed-responsive-16by9"> <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/BrJSRlAJaX4" allowfullscreen=""></iframe> </div> <hr/> <h1 id="presentation">Presentation</h1> <p>Video presentation at <a href="https://leggedrobots.org/">“ICRA 2022 - 6th Full-Day Workshop on Legged Robots”</a>.</p> <div class="embed-responsive embed-responsive-16by9"> <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/CGtzHkMmNuY" allowfullscreen=""></iframe> </div> <hr/> <h1 id="demo">Demo</h1> <div class="embed-responsive embed-responsive-16by9"> <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/J9GfVgz80lk" allowfullscreen=""></iframe> </div> <p><br/></p> <div class="embed-responsive embed-responsive-16by9"> <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/D99qXOs9uQs" allowfullscreen=""></iframe> </div> <p><br/></p> <div class="embed-responsive embed-responsive-16by9"> <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/rUv4jwwKFiY" allowfullscreen=""></iframe> </div> <p><br/></p> <div class="embed-responsive embed-responsive-16by9"> <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/NCCBZf-Nuas" allowfullscreen=""></iframe> </div> <hr/> <h1 id="bibtex">Bibtex</h1> <figure class="highlight"><pre><code class="language-txt" data-lang="txt">@misc{kang2022nonlinear,
  title={Nonlinear Model Predictive Control for Quadrupedal Locomotion Using Second-Order Sensitivity Analysis}, 
  author = {Kang, Dongho and De Vincenti, Flavio and Coros, Stelian},
  url = {https://arxiv.org/abs/2207.10465},
  year={2022},
  eprint={2207.10465},
  archivePrefix={arXiv},
  primaryClass={cs.RO},
}</code></pre></figure> <hr/> <h1 id="acknowledgment">Acknowledgment</h1> <p>This work has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No. 866480.)</p>]]></content><author><name>Dongho Kang</name></author><category term="publication"/><category term="robotics"/><category term="arxiv"/><summary type="html"><![CDATA[ICRA 2022 - 6th Full-Day Workshop on Legged Robots]]></summary></entry><entry><title type="html">Animal Gaits on Quadrupedal Robots Using Motion Matching and Model-Based Control</title><link href="https://kangdongho.me/animal-gaits-on-quadrupedal-robots/" rel="alternate" type="text/html" title="Animal Gaits on Quadrupedal Robots Using Motion Matching and Model-Based Control"/><published>2021-08-04T14:44:33+00:00</published><updated>2021-08-04T14:44:33+00:00</updated><id>https://kangdongho.me/animal-gaits-on-quadrupedal-robots</id><content type="html" xml:base="https://kangdongho.me/animal-gaits-on-quadrupedal-robots/"><![CDATA[<div class="row"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid" src="/assets/img/kang2021animal/teaser.png" alt="" title="teaser"/> </div> </div> <p><br/></p> <h1 id="abstract">Abstract</h1> <p>In this paper, we explore the challenge of generating animal-like walking motions for legged robots. To this end, we propose a versatile and robust control pipeline that combines a state-of-the-art model-based controller with a data-driven technique that is commonly used in computer animation. We demonstrate the efficacy of our control framework on a variety of quadrupedal robots in simulation. We show, in particular, that our approach can automatically reproduce key characteristics of animal motions, including speed-specific gaits, unscripted footfall patterns for nonperiodic motions, and natural small variations in overall body movements.</p> <p><strong>Paper: [<a href="https://ieeexplore.ieee.org/document/9635838">IEEE Xplore</a>]</strong>   <strong>Open access: [<a href="https://www.research-collection.ethz.ch/handle/20.500.11850/528453">ETH Research Collection</a>]</strong></p> <p><strong>News:</strong> our supplementary video of “Animal Gaits on Quadrupedal Robots Using Motion Matching and Model Based Control” has been selected for <a href="https://spectrum.ieee.org/video-friday-robot-opera">IEEE Spectrum Video Friday collection</a> on August 06th 2021!</p> <hr/> <h1 id="supplementary-video">Supplementary Video</h1> <div class="embed-responsive embed-responsive-16by9"> <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/6-zTPTL0fJY" allowfullscreen=""></iframe> </div> <hr/> <h1 id="bibtex">Bibtex</h1> <figure class="highlight"><pre><code class="language-txt" data-lang="txt">@inproceedings{kang2021animal,
  title={Animal Gaits on Quadrupedal Robots Using Motion Matching and Model-Based Control},
  author={Kang, Dongho and Zimmermann, Simon and Coros, Stelian},
  booktitle={2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={8500--8507},
  year={2021},
  organization={IEEE},
  doi={10.1109/IROS51168.2021.9635838}
}</code></pre></figure> <hr/> <h1 id="acknowledgment">Acknowledgment</h1> <p>This work has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No. 866480.)</p>]]></content><author><name>Dongho Kang</name></author><category term="publication"/><category term="robotics"/><category term="character animation"/><summary type="html"><![CDATA[2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2021)]]></summary></entry><entry><title type="html">UnityMeshImporter</title><link href="https://kangdongho.me/UnityMeshImporter/" rel="alternate" type="text/html" title="UnityMeshImporter"/><published>2020-04-12T09:38:33+00:00</published><updated>2020-04-12T09:38:33+00:00</updated><id>https://kangdongho.me/unity-mesh-importer</id><content type="html" xml:base="https://kangdongho.me/UnityMeshImporter/"><![CDATA[<p> Unity does not support run-time mesh import from files outside of the project for performance. Fortunately, <a href="https://bitbucket.org/Starnick/assimpnet">AssimpNet</a> provides nice Unity plugin enables loading various formats of mesh objects. </p> <p> I wrote a Unity package based on this: <b>UnityMeshImporter</b>. This package loads mesh file by AssimpNet and convert it to a Unity GameObject. The source code is available on <a href="https://github.com/eastskykang/UnityMeshImporter">GitHub</a>. </p> <p> It just works like this: </p> <figure class="highlight"><pre><code class="language-c#" data-lang="c#"><span class="k">using</span> <span class="nn">UnityMeshImporter</span><span class="p">;</span>

<span class="kt">string</span> <span class="n">meshFile</span> <span class="p">=</span> <span class="s">"YOUR-MESH-FILE-PATH"</span><span class="p">;</span>
<span class="kt">var</span> <span class="n">ob</span> <span class="p">=</span> <span class="n">MeshImporter</span><span class="p">.</span><span class="nf">Load</span><span class="p">(</span><span class="n">meshFile</span><span class="p">);</span> </code></pre></figure> <br> <p> <img class="img-fluid rounded z-depth-1" src="/assets/img/kang2020unity/unity-mesh-importer.gif" alt="raisim-zerogravity-gif" title="Zero gravity test"> </p> <p> Very simple! </p> <p> You can find a simple example Unity app <a href="https://github.com/eastskykang/UnityMeshImportExample">here.</a> Please star the projects if you like it. Any comments or issue rerporting through GitHub issue is welcomed. This project is also used for <a href="https://github.com/leggedrobotics/RaiSimUnity">RaiSimUnity</a> project. </p>]]></content><author><name>Dongho Kang</name></author><category term="software"/><category term="graphics"/><summary type="html"><![CDATA[A runtime mesh importer for Unity]]></summary></entry><entry><title type="html">SimBenchmark</title><link href="https://kangdongho.me/blog/2018/simbenchmark/" rel="alternate" type="text/html" title="SimBenchmark"/><published>2018-06-08T16:08:45+00:00</published><updated>2018-06-08T16:08:45+00:00</updated><id>https://kangdongho.me/blog/2018/simbenchmark</id><content type="html" xml:base="https://kangdongho.me/blog/2018/simbenchmark/"><![CDATA[<div class="row"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid" src="/assets/img/kang2018simbenchmark/simbenchmark.png" alt="" title="teaser"/> </div> </div> <p><br/> SimBenchmark is a benchmark suite for state-of-the-art physics engines. We assessed several widely used physics engines for robotics and machine learning applications:</p> <ul> <li>RAISIM</li> <li>Bullet Physics</li> <li>Open Dynamics Engine</li> <li>Multi-Joint dynamics with Contact (a.k.a. MuJoCo)</li> <li>DART Sim</li> </ul> <p>More details are available at the <a href="https://leggedrobotics.github.io/SimBenchmark/">SimBenchmark website</a></p>]]></content><author><name>Dongho Kang</name></author><category term="software"/><category term="robotics"/><category term="physics simulation"/><summary type="html"><![CDATA[A benchmark suite for physics engines]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://kangdongho.me/%5Cimg/kang2018simbenchmark/simbenchmark.png"/><media:content medium="image" url="https://kangdongho.me/%5Cimg/kang2018simbenchmark/simbenchmark.png" xmlns:media="http://search.yahoo.com/mrss/"/></entry><entry><title type="html">RaiSim</title><link href="https://kangdongho.me/2018/02/24/raisim/" rel="alternate" type="text/html" title="RaiSim"/><published>2018-02-24T12:01:45+00:00</published><updated>2018-02-24T12:01:45+00:00</updated><id>https://kangdongho.me/2018/02/24/raisim-effcient-pipeline-for-rigid-body-simulation-with-contacts</id><content type="html" xml:base="https://kangdongho.me/2018/02/24/raisim/"><![CDATA[<blockquote> <p> From 2020, I have not taken part in RaiSim project anymore. However, RaiSim is still under active development by <a href="https://www.railab.kaist.ac.kr/">Prof. Jemin Hwango</a> and it's distributed under proprietary license from August, 2020. Please visit <a href="http://raisim.com/">raisim.com</a> for more details and up-to-date documentation. </p> </blockquote> <p> <b>RaiSim</b> is a rigid body simulator developed by researchers at <a href="http://www.rsl.ethz.ch/">RSL, ETH Zurich</a> (Jemin Hwangbo and Dongho Kang). The goal of RaiSim project is to provide simulation for data-driven robotics and animation research. </p> <div class="row"> <div class="col-sm mt-2 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/kang2018raisim/raisim-zerogravity.gif" alt="raisim-zerogravity-gif" title="Zero gravity test"> </div> <div class="col-sm mt-2 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/kang2018raisim/raisim-anymals.gif" alt="raisim-anymals-gif" title="ANYmal test"> </div> </div> <br> <h1>Abstract</h1> <p> Solving contact dynamics is a NP-hard problem due to its non-convexity and discontinuity. In order to make the problem tractable, rigid-body simulators often over-relax the contact problem that might leads to inaccurate result. In this project, the multi-contact simulation pipeline <i>RaiSim</i> was implemented based on <a href="http://ieeexplore.ieee.org/abstract/document/8255551/">Hwangbo’s method</a> that results in accurate solution in an efficient manner. By using Hwangbo’s method, the penetration problem is significantly reduced. Moreover, RaiSim obtains contact solution using sparse formulation of contact problem thus fast and memory-efficient. For the integration, RaiSim computes gyroscopic force by Implicit Euler scheme and integrate the object velocity by Semi-implicit Euler that improves numerical stability of the rigid-body simulation. In the benchmark tests, our simulation pipeline performed similarly or better than the state-of-the-art simulators in terms of speed and better in terms of accuracy. Remarkably, it can simulate 18-DOF quadruped robot at 29k time step per second in a 12 contact points case. This performance could be achievable by the virtue of accurate and fast solver, sparse representation of the contact problem, efficient memory management and numerically stable integration scheme. </p> <p> <i>Dongho Kang, "RaiSim: Efficient Pipeline for Rigid Body Simulation with Contacts," MSc Semester Thesis, Departement Maschinenbau und Verfahrenstechnik, ETH Zürich, Jan. 2018</i> </p> <br> <div class="embed-responsive embed-responsive-16by9"> <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/Xw3bsh0tWFU" allowfullscreen></iframe> </div>]]></content><author><name>Dongho Kang</name></author><category term="software"/><category term="robotics"/><category term="physics simulation"/><summary type="html"><![CDATA[An efficient pipeline for rigid body simulation with contacts]]></summary></entry></feed>